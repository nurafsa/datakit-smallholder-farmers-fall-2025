{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0edad6b0-81ca-458f-871a-f7488027e29d",
   "metadata": {},
   "source": [
    "# PREDICT VALUE FOR MISSING QUESTION TOPIC WITH RELABELLING V2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346a5ef8-99bd-43a8-83a0-b4c184034fb0",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c538316c-9abf-479f-8e67-0b56afd4637f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-07 22:06:34.494721: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1765105594.581336   16397 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1765105594.609180   16397 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1765105594.670207   16397 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1765105594.670335   16397 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1765105594.670338   16397 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1765105594.670339   16397 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_MIN_GPU_MULTIPROCESSOR_COUNT'] = '6' # Needed so I can use my old GPU with the new one\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0' # Turns off oneDNN custom operations\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1' # Hides message regarding TensorFlow optimization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2991ae-3112-4dde-8da6-0d5b7daa2d32",
   "metadata": {},
   "source": [
    "## Import CSV File & Remove Duplicates\n",
    "NB:\n",
    "\n",
    "'question_topic_valid.csv' represents records from the original dataset where 'question_topic' was not empty while 'question_topic_null.csv' are records with missing values for 'question_topic'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cbb4951-125c-42ab-9767-4b03b73c326d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import patitioned libraries of original dataset\n",
    "# These datasets were created in another notebook (see 'data_wrangling_new_features.ipynb')\n",
    "df_topic_exists = pd.read_csv('../data/question_topic_valid_relabel_r2.csv', usecols=[0,2,3,4,13,14]) # Import only essential columns\n",
    "df_topic_null = pd.read_csv('../data/question_topic_null.csv')\n",
    "\n",
    "# Drops duplicate 'question_content' due to multiple 'question_id' in the dataset\n",
    "df_topic_exists.drop_duplicates(subset='question_id',inplace=True)\n",
    "df_topic_exists.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feafbd64-c133-4cf5-b57a-e331a75582b7",
   "metadata": {},
   "source": [
    "## Tokenize The Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "243fcdda-d3c0-4493-bfe0-09c99ac6a005",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_of_sentences(col):\n",
    "    sentence_list = []\n",
    "    for text in col:\n",
    "        splitted_text = text.lower().split()\n",
    "        sentence_list.append(splitted_text)\n",
    "    return sentence_list\n",
    "\n",
    "sentences_topic_exist = list_of_sentences(df_topic_exists.question_content)\n",
    "sentences_topic_null = list_of_sentences(df_topic_null.question_content)\n",
    "\n",
    "# Initiate the tokenizer with a out_of_vocabulary token \n",
    "tokenizer_X = Tokenizer(oov_token=\"<OOV>\")\n",
    "\n",
    "# Generate word indexes for all sentences \n",
    "tokenizer_X.fit_on_texts(sentences_topic_exist+sentences_topic_null)\n",
    "\n",
    "# Generate separate sequences for both with topic values and missing values\n",
    "X = tokenizer_X.texts_to_sequences(sentences_topic_exist)\n",
    "X_topic_null = tokenizer_X.texts_to_sequences(sentences_topic_null)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b6956b-9f6c-4a3d-8767-807e1aeb5ce1",
   "metadata": {},
   "source": [
    "## Determine Word Counts & Maximum Sentence Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f803052c-b3ca-4469-92e3-bd784941bd10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of words from all questions is 1292953.\n",
      "The highest number of words in any sentence is 197.\n"
     ]
    }
   ],
   "source": [
    "print(f'The total number of words from all questions is {len(tokenizer_X.word_counts)}.')\n",
    "\n",
    "max_len = 0\n",
    "for l in X + X_topic_null: # Include questions from the entire dataset\n",
    "    if len(l) > max_len:\n",
    "        max_len = len(l)\n",
    "\n",
    "print(f'The highest number of words in any sentence is {max_len}.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4942d2bf-9b4c-4dd7-9e4f-240485a8d914",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 40000     # Use 40000 most frequent words from the total of 1292953 words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5993d23-c057-4b78-b556-5cd8d536268b",
   "metadata": {},
   "source": [
    "## Create Train & Test Datasets & Prepare For Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "315e1535-0d21-40bb-8425-885f79bc7206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test sets\n",
    "X_train_df, X_test_df, y_train_df, y_test_df = train_test_split(X, df_topic_exists.question_topic, test_size=0.2,\n",
    "                                                                stratify=df_topic_exists.question_topic, random_state=42)\n",
    "\n",
    "# Format X and y for model\n",
    "X_train = np.array(sequence.pad_sequences(X_train_df, maxlen=max_len))\n",
    "X_test = np.array(sequence.pad_sequences(X_test_df, maxlen=max_len))\n",
    "\n",
    "y_train_one_hot = pd.get_dummies(y_train_df)\n",
    "y_train = y_train_one_hot.to_numpy()\n",
    "y_test_one_hot = pd.get_dummies(y_test_df)\n",
    "y_test = y_test_one_hot.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82eb8495-742f-4d35-ab60-7ffd05c21821",
   "metadata": {},
   "source": [
    "## Configure Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f680d6e-d724-4ba7-b20d-fdd862fbf8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create transformer block class\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.01):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            layers.Dense(ff_dim, activation='relu'),\n",
    "            layers.Dense(embed_dim)\n",
    "        ])\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87e8d561-5861-4b89-86e1-c73ef6064639",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1765105989.222164   16397 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5518 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Ti, pci bus id: 0000:01:00.0, compute capability: 8.9\n",
      "I0000 00:00:1765105989.224889   16397 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 2857 MB memory:  -> device: 1, name: NVIDIA GeForce GTX 1050 Ti, pci bus id: 0000:07:00.0, compute capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "# Define the model with an embedding layer, transformer block, and output layer\n",
    "embed_dim = 32 # Embedding dimension for each word vector\n",
    "num_heads = 4  # The number of attention heads in the multi-head attention layer\n",
    "ff_dim = 64    # Number of units in the feed forward layer\n",
    "\n",
    "inputs = layers.Input(shape=(max_len,))\n",
    "\n",
    "embedding_layer = layers.Embedding(input_dim=max_features, output_dim=embed_dim)\n",
    "out = embedding_layer(inputs)\n",
    "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "out = transformer_block(out, training=True)\n",
    "out = layers.GlobalAveragePooling1D()(out)\n",
    "out = layers.Dropout(0.1)(out)\n",
    "out = layers.Dense(20, activation='relu')(out)\n",
    "out = layers.Dropout(0.1)(out)\n",
    "outputs = layers.Dense(148, activation='softmax')(out)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73569ba3-a84f-4fe5-b6cc-0d6145c09d6a",
   "metadata": {},
   "source": [
    "## Compile & Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "945f82e3-e324-47d5-a88e-835800da38c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-07 22:13:12.133844: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 2115021944 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1765105998.814131   16696 service.cc:152] XLA service 0x705480007360 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1765105998.814198   16696 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 4060 Ti, Compute Capability 8.9\n",
      "I0000 00:00:1765105998.814203   16696 service.cc:160]   StreamExecutor device (1): NVIDIA GeForce GTX 1050 Ti, Compute Capability 6.1\n",
      "I0000 00:00:1765105999.387329   16696 cuda_dnn.cc:529] Loaded cuDNN version 91002\n",
      "I0000 00:00:1765106019.016249   16696 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5243/5243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m256s\u001b[0m 44ms/step - accuracy: 0.8485 - loss: 0.6461 - val_accuracy: 0.9186 - val_loss: 0.2867\n",
      "Epoch 2/5\n",
      "\u001b[1m5243/5243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m205s\u001b[0m 39ms/step - accuracy: 0.9127 - loss: 0.3183 - val_accuracy: 0.9206 - val_loss: 0.2619\n",
      "Epoch 3/5\n",
      "\u001b[1m5243/5243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m205s\u001b[0m 39ms/step - accuracy: 0.9161 - loss: 0.2865 - val_accuracy: 0.9209 - val_loss: 0.2537\n",
      "Epoch 4/5\n",
      "\u001b[1m5243/5243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m205s\u001b[0m 39ms/step - accuracy: 0.9184 - loss: 0.2677 - val_accuracy: 0.9218 - val_loss: 0.2465\n",
      "Epoch 5/5\n",
      "\u001b[1m5243/5243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m205s\u001b[0m 39ms/step - accuracy: 0.9203 - loss: 0.2546 - val_accuracy: 0.9218 - val_loss: 0.2445\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x705661a26e90>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=512, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17dd374-2685-4c79-86ea-3e9bdc6407f5",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b54c3f34-1aeb-4524-bcb5-92f63cd4f991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m26212/26212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m180s\u001b[0m 7ms/step - accuracy: 0.9218 - loss: 0.2461\n",
      "Test Accuracy: 0.9217656254768372\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(\"Test Accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b83c435-514d-414d-8502-f4137e57f446",
   "metadata": {},
   "source": [
    "## Extract Failed Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "807fcf2b-18d8-4695-9d1d-ce6a273290a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m26212/26212\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 4ms/step\n"
     ]
    }
   ],
   "source": [
    "# Store in a list the column names for one-hot encoding (question_topic)\n",
    "one_hot_columns = list(y_test_one_hot.columns)\n",
    "\n",
    "# Store predictions for X_test\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Add predictions column to y_test_df\n",
    "y_test_df = y_test_df.to_frame()\n",
    "y_test_df['predictions'] = [one_hot_columns[i] for i in np.argmax(y_pred, axis=1)]\n",
    "\n",
    "# Merge index associated rows from the original source dataset along with the predictions \n",
    "test_df = pd.merge(df_topic_exists, y_test_df, left_index=True, right_index=True)\n",
    "\n",
    "# Create new dataframe that stores rows from test df where predictions were incorrect plus adds the predictions column\n",
    "false_predictions = pd.DataFrame()\n",
    "for i,v in test_df.iterrows():\n",
    "    if v.question_topic_x != v.predictions:\n",
    "        row = pd.DataFrame({'question_language' : [v.question_language], 'question_content' : [v.question_content],\n",
    "                            'question_user_status' : [v.question_user_status], 'question_user_country_code' : [v.question_user_country_code],\n",
    "                            'question_topic': [v.question_topic_x], 'predictions' : [v.predictions]\n",
    "                            })\n",
    "        false_predictions = pd.concat([false_predictions, row], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01507607-286b-4f4b-94a4-b8d4add2b628",
   "metadata": {},
   "source": [
    "# Export Test df For Predictions Versus Actual Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02586d05-fa9f-4a02-8847-fb6e5faaf178",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv('../data/prediction_vs_actual_topic_r2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace49208-9f2c-4afe-b1f7-78da0638fc4a",
   "metadata": {},
   "source": [
    "## Check For Any Indicators For Failure Rate"
   ]
  },
  {
   "cell_type": "raw",
   "id": "92605c71-b205-4886-9f8c-8a5b2ce50567",
   "metadata": {},
   "source": [
    "The overall failure rate for the test data set after more aggressive relabelling is 7.82%. An almost 2% improvement on the original test dataset before any relabelling was performed. This should be expected as the algorithm used for this testing looks for an association between the question content and question topic. As the relabelling of the question topic is based on the question content, it should strengthen the association between the two when testing the accuracy of the test dataset.\n",
    "\n",
    "More important is the error rate for the question topic of 'plant'. Previously it has been just over 40%. After this more aggressive labelling was performed, it is now down to 15.30%. This suggests that the relabelling may have been justified.\n",
    "\n",
    "For question_language, and question_user_status, and question_user_country_code the percentage failure rates are fairly consistent with the overall failure rate.\n",
    "\n",
    "The topics in question_topic with lower failure rates tended to have higher counts although there were some values that performed well despite the very small sample size. The topic 'maize' now performs best with a failure rate of 2.17%. There were 20 topics with a sample size less than 60 that could not make even 1 correct prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9cac0a96-50a4-45de-a6f6-5ec092e79ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test data % failure rate by question_language\n",
      "eng     6.288004\n",
      "lug     9.215363\n",
      "nyn     6.350424\n",
      "swa    10.070384\n",
      "Name: count, dtype: float64\n",
      "\n",
      "The test data % failure rate by question_user_country_code\n",
      "ke    9.148681\n",
      "ug    6.301055\n",
      "tz    7.397424\n",
      "gb    6.666667\n",
      "Name: count, dtype: float64\n",
      "\n",
      "The test data % failure rate by question_user_status\n",
      "live         7.505945\n",
      "zombie       8.530012\n",
      "destroyed    8.345033\n",
      "blocked      8.276858\n",
      "Name: count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(f'The test data % failure rate by {(false_predictions.question_language.value_counts() / test_df.question_language.value_counts()) * 100}\\n')\n",
    "print(f'The test data % failure rate by {(false_predictions.question_user_country_code.value_counts() / test_df.question_user_country_code.value_counts()) * 100}\\n')\n",
    "print(f'The test data % failure rate by {(false_predictions.question_user_status.value_counts() / test_df.question_user_status.value_counts()) * 100}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3681182e-40b8-4790-8532-e3188a7d8da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 60 failure rates \n",
      "         question_topic  failed_prediction  total  percentage_failed\n",
      "0            courgette                 40     40         100.000000\n",
      "1                 flax                 53     53         100.000000\n",
      "2           gooseberry                 27     27         100.000000\n",
      "3           guinea-pig                 33     33         100.000000\n",
      "4          castor-bean                  6      6         100.000000\n",
      "5                  rye                  6      6         100.000000\n",
      "6           blackberry                  4      4         100.000000\n",
      "7            cranberry                  1      1         100.000000\n",
      "8              setaria                  3      3         100.000000\n",
      "9                vetch                  4      4         100.000000\n",
      "10            mulberry                  4      4         100.000000\n",
      "11            leucaena                  4      4         100.000000\n",
      "12        purple-vetch                  1      1         100.000000\n",
      "13              celery                 11     11         100.000000\n",
      "14             apricot                  7      7         100.000000\n",
      "15            snap-pea                  9      9         100.000000\n",
      "16               lupin                  6      6         100.000000\n",
      "17           asparagus                 11     11         100.000000\n",
      "18            chickpea                 14     14         100.000000\n",
      "19               chard                 17     17         100.000000\n",
      "20               peach                100    107          93.457944\n",
      "21    black-nightshade                 29     34          85.294118\n",
      "22                pear                162    224          72.321429\n",
      "23                leek                  9     13          69.230769\n",
      "24              acacia                 84    128          65.625000\n",
      "25      collard-greens                 70    108          64.814815\n",
      "26           safflower                 89    144          61.805556\n",
      "27               guava                171    282          60.638298\n",
      "28              clover                 84    139          60.431655\n",
      "29       finger-millet                 87    151          57.615894\n",
      "30              barley                 39     72          54.166667\n",
      "31              cereal               1628   3011          54.068416\n",
      "32              radish                 38     78          48.717949\n",
      "33              sesame                 28     60          46.666667\n",
      "34        napier-grass                878   1917          45.800730\n",
      "35            plantain               1475   3386          43.561725\n",
      "36              greens                 99    228          43.421053\n",
      "37               wheat               2064   5036          40.984909\n",
      "38               sisal                138    341          40.469208\n",
      "39            amaranth                 50    125          40.000000\n",
      "40        sweet-potato               1052   2664          39.489489\n",
      "41               melon                948   2406          39.401496\n",
      "42               olive                323    826          39.104116\n",
      "43           aubergine                505   1388          36.383285\n",
      "44         cauliflower                  9     27          33.333333\n",
      "45              pigeon                472   1464          32.240437\n",
      "46            snow-pea                 66    216          30.555556\n",
      "47             parsley                 64    212          30.188679\n",
      "48              garlic                214    720          29.722222\n",
      "49          eucalyptus                133    462          28.787879\n",
      "50              squash                 59    210          28.095238\n",
      "51               lemon                 81    292          27.739726\n",
      "52            capsicum                751   2758          27.229877\n",
      "53              lucern                 20     74          27.027027\n",
      "54           jackfruit                 49    187          26.203209\n",
      "55                 cat                308   1192          25.838926\n",
      "56  african-nightshade                135    533          25.328330\n",
      "57             ostrich                 26    103          25.242718\n",
      "58                bird                712   2940          24.217687\n",
      "59         sudan-grass                  5     21          23.809524\n",
      "\n",
      "The middle 28 failure rates \n",
      "       question_topic  failed_prediction  total  percentage_failed\n",
      "60         desmodium                 25    107          23.364486\n",
      "61  brachiaria-grass                 16     69          23.188406\n",
      "62          rapeseed                 35    153          22.875817\n",
      "63           tilapia                 98    455          21.538462\n",
      "64            turkey                221   1118          19.767442\n",
      "65            bamboo                 27    140          19.285714\n",
      "66            cowpea                235   1223          19.215045\n",
      "67               oat                 12     65          18.461538\n",
      "68             grass                559   3034          18.424522\n",
      "69           poultry               9314  53965          17.259335\n",
      "70             camel                102    625          16.320000\n",
      "71         mung-bean                 80    491          16.293279\n",
      "72              duck                681   4370          15.583524\n",
      "73        nightshade                 12     78          15.384615\n",
      "74         livestock               1085   7058          15.372627\n",
      "75             plant               2622  17133          15.303800\n",
      "76         pyrethrum                 35    231          15.151515\n",
      "77           coconut                 55    367          14.986376\n",
      "78           lettuce                  4     27          14.814815\n",
      "79          broccoli                  5     36          13.888889\n",
      "80            orange                242   1749          13.836478\n",
      "81            chilli                195   1476          13.211382\n",
      "82       boma-rhodes                 31    236          13.135593\n",
      "83        strawberry                 29    227          12.775330\n",
      "84            animal               1364  10696          12.752431\n",
      "85           paw-paw                159   1278          12.441315\n",
      "86               pea                192   1546          12.419146\n",
      "87       guinea-fowl                175   1414          12.376238\n",
      "\n",
      "The bottom 60 failure rates \n",
      "        question_topic  failed_prediction   total  percentage_failed\n",
      "88               okra                 22     194          11.340206\n",
      "89          caliandra                  1       9          11.111111\n",
      "90               tree                558    5189          10.753517\n",
      "91          macademia                 32     302          10.596026\n",
      "92               taro                 60     574          10.452962\n",
      "93             millet                481    4610          10.433839\n",
      "94             cyprus                 13     126          10.317460\n",
      "95             potato               2010   20040          10.029940\n",
      "96             cotton               1209   12371           9.772856\n",
      "97   butternut-squash                 67     692           9.682081\n",
      "98               goat               2262   23961           9.440341\n",
      "99            spinach                176    1884           9.341826\n",
      "100          cucumber                 20     215           9.302326\n",
      "101               yam                 67     734           9.128065\n",
      "102           tobacco                 94    1040           9.038462\n",
      "103             sheep                732    8192           8.935547\n",
      "104          beetroot                 38     429           8.857809\n",
      "105             cocoa                 56     642           8.722741\n",
      "106              crop               1471   17133           8.585770\n",
      "107         vegetable                501    5903           8.487210\n",
      "108          mushroom                123    1464           8.401639\n",
      "109               dog                256    3128           8.184143\n",
      "110           avocado                242    2966           8.159137\n",
      "111        sugar-cane                422    5250           8.038095\n",
      "112               pig               1993   24843           8.022381\n",
      "113             miraa                 22     309           7.119741\n",
      "114        watermelon                575    8167           7.040529\n",
      "115           pumpkin                 90    1296           6.944444\n",
      "116             mango                179    2589           6.913866\n",
      "117            ginger                103    1508           6.830239\n",
      "118              chia                 13     201           6.467662\n",
      "119        pigeon-pea                 30     465           6.451613\n",
      "120       french-bean                 39     606           6.435644\n",
      "121           chicken               5509   86472           6.370848\n",
      "122         pineapple                 91    1466           6.207367\n",
      "123     passion-fruit                463    7537           6.143028\n",
      "124              kale                352    5753           6.118547\n",
      "125             onion               1019   16873           6.039234\n",
      "126        cashew-nut                 69    1172           5.887372\n",
      "127              bean               2053   35146           5.841348\n",
      "128           cassava                326    5793           5.627481\n",
      "129              soya                104    1914           5.433647\n",
      "130             apple                 58    1074           5.400372\n",
      "131            peanut                394    7380           5.338753\n",
      "132            carrot                206    4094           5.031754\n",
      "133         sunflower                158    3275           4.824427\n",
      "134        corriander                  3      63           4.761905\n",
      "135            locust                  5     110           4.545455\n",
      "136              fish                217    4810           4.511435\n",
      "137             grape                  5     117           4.273504\n",
      "138               tea                194    4882           3.973781\n",
      "139               bee                223    5836           3.821110\n",
      "140            cattle               2742   85302           3.214462\n",
      "141              rice                468   15295           3.059823\n",
      "142            rabbit                476   16927           2.812075\n",
      "143            coffee                507   18174           2.789700\n",
      "144            tomato               1759   67234           2.616236\n",
      "145            banana                400   16239           2.463206\n",
      "146           cabbage                307   13106           2.342439\n",
      "147             maize               2430  111779           2.173932\n"
     ]
    }
   ],
   "source": [
    "question_topic_failed = false_predictions.question_topic.value_counts().rename_axis('question_topic').reset_index(name='failed_prediction')\n",
    "question_topic_total = test_df.question_topic_x.value_counts().rename_axis('question_topic').reset_index(name='total')\n",
    "question_topic = pd.merge(question_topic_failed, question_topic_total, how='inner')\n",
    "question_topic['percentage_failed'] = (question_topic['failed_prediction'] / question_topic['total']) * 100\n",
    "question_topic = question_topic.sort_values(by=['percentage_failed'],ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(f'The top 60 failure rates \\n {question_topic.head(60)}\\n')\n",
    "print(f'The middle 28 failure rates \\n {question_topic[60:88]}\\n')\n",
    "print(f'The bottom 60 failure rates \\n {question_topic.tail(60)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141a48a3-5e29-4bc9-ae29-c85c19c49387",
   "metadata": {},
   "source": [
    "# Free Memory For Next Step\n",
    "\n",
    "NB: Optional step if system resources are limited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec2735ac-115c-478a-b46d-6c770a11f30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %xdel sentences_topic_exist\n",
    "# %xdel X \n",
    "# %xdel X_train_df\n",
    "# %xdel X_test_df\n",
    "# %xdel y_train_df\n",
    "# %xdel y_test_df\n",
    "# %xdel X_train\n",
    "# %xdel X_test\n",
    "# %xdel y_train_one_hot\n",
    "# %xdel y_train\n",
    "# %xdel y_test_one_hot\n",
    "# %xdel y_test\n",
    "# %xdel y_pred\n",
    "# %xdel false_predictions\n",
    "# %xdel df_topic_exists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1a5ba5-a931-45b1-bbd4-096fc8d4c008",
   "metadata": {},
   "source": [
    "## Make Predictions For Missing question_topic Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2052611-2997-4b2a-b43e-da411a9cf572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m     2/110555\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:53:01\u001b[0m 94ms/step"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-07 22:52:05.196307: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 2787730452 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m110555/110555\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m466s\u001b[0m 4ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-07 23:00:45.392459: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 2094335568 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "# Create X input and make predictions\n",
    "X_topic_null_predict = np.array(sequence.pad_sequences(X_topic_null, maxlen=max_len))\n",
    "y_pred_topic_null = model.predict(X_topic_null_predict)\n",
    "\n",
    "# Convert predictions to labels\n",
    "topic_null_predictions = [one_hot_columns[i] for i in np.argmax(y_pred_topic_null, axis=1)]\n",
    "\n",
    "# Insert predictions into 'question_topic' column for null dataframe\n",
    "df_topic_null['question_topic'] = topic_null_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0555e7d-66bb-4a23-9162-e167b3c35d03",
   "metadata": {},
   "source": [
    "# Free Memory For Next Step\n",
    "\n",
    "NB: Optional step if system resources are limited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f653521-2b59-4fec-b162-158963a4a58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %xdel X_topic_null_predict\n",
    "# %xdel y_pred_topic_null\n",
    "# %xdel topic_null_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b97247-398a-40fe-856e-603baa6ce01f",
   "metadata": {},
   "source": [
    "# Export To CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ccf1aa7d-d629-4264-b110-54dee3e903e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import full datset without missing values for 'question_topic'\n",
    "# NB: Could not do this before due to resource limit on my computer\n",
    "chunks = pd.read_csv('../data/question_topic_valid.csv',\n",
    "                     dtype={'question_user_gender': str, 'response_user_gender': str}, # Removes mixed dtypes error message\n",
    "                     chunksize=100000\n",
    "                    )\n",
    "df_topic_exists = pd.DataFrame()\n",
    "\n",
    "for chunk in chunks:\n",
    "    df_topic_exists = pd.concat([df_topic_exists,chunk], axis=0)\n",
    "\n",
    "\n",
    "# Combine dataset without missing values with the predicted values to recreate the full dataset\n",
    "df_no_missing = pd.concat([df_topic_exists, df_topic_null], axis=0)\n",
    "\n",
    "\n",
    "# Export the predicted values only and the full dataset now with no missing values\n",
    "df_topic_null.to_csv('../data/question_topic_predicted_r2.csv', index=False)\n",
    "df_no_missing.to_csv('../data/question_topic_no_missing_r2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b881cf-c05a-45a0-9faa-f70c4aef8730",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
